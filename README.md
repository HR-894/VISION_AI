# ğŸ‘ï¸ VISION AI

**Your Intelligent AI Voice & Text Assistant for Windows**

100% Local â€¢ Privacy-First â€¢ Adaptive AI â€¢ Multi-Step Planning

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Platform: Windows](https://img.shields.io/badge/platform-Windows-lightgrey.svg)](https://www.microsoft.com/windows)

---

## ğŸŒŸ Features

- ğŸ¤ **Voice Commands** - Whisper AI (90-95% accuracy)
- ğŸ§  **LLM Planning** - Multi-step task automation with Llama
- ğŸ”’ **100% Private** - All AI runs locally on YOUR device
- âš¡ **Adaptive Performance** - Auto-optimizes for your hardware
- ğŸ§  **Smart Memory** - Learns from your commands
- ğŸŒ **Web Search** - DuckDuckGo integration (optional)
- ğŸ“‚ **File Management** - List, search, open files
- ğŸ¬ **YouTube Automation** - Voice-controlled browsing
- ğŸªŸ **Window Controls** - Manage apps, take screenshots
- ğŸ’» **System Monitoring** - CPU, RAM, and more

---

## ğŸ“‹ Requirements

### Minimum System Requirements
- **OS:** Windows 10/11 (64-bit)
- **RAM:** 4GB (8GB+ recommended)
- **Storage:** 2GB free space
- **Python:** 3.10 or higher (for running from source)

### For .exe Distribution (No Python needed!)
- Just Windows 10/11 64-bit
- 2GB free space

---

## ğŸš€ Quick Start (Using .exe)

### Option 1: Pre-built Executable

1. **Download** the `VISION_AI.zip` from releases
2. **Extract** to any folder
3. **Download Models** (first time only):
   ```powershell
   # Create models folder
   cd VISION_AI
   mkdir models
   
   # Download Llama model (770MB)
   # Visit: https://huggingface.co/TheBloke/Llama-3.2-1B-Instruct-GGUF
   # Download: Llama-3.2-1B-Instruct-Q4_K_M.gguf
   # Place in: models/
   ```
4. **Run** `VISION_AI.exe`

**First launch will:**
- Detect your hardware
- Load Whisper model (auto-download ~1GB)
- Initialize LLM
- Start GUI (~30 seconds total)

---

## ğŸ› ï¸ Installation (From Source)

### Step 1: Clone Repository

```bash
# Clone the repo
git clone https://github.com/YOUR_USERNAME/VISION_AI.git
cd VISION_AI
```

### Step 2: Create Virtual Environment

```bash
# Create virtual environment
python -m venv .venv

# Activate (Windows PowerShell)
.\.venv\Scripts\Activate.ps1

# Activate (Windows CMD)
.venv\Scripts\activate.bat

# Activate (Git Bash)
source .venv/Scripts/activate
```

### Step 3: Install Dependencies

```bash
# Upgrade pip
python -m pip install --upgrade pip

# Install all requirements
pip install -r requirements.txt

# This installs:
# - customtkinter (GUI)
# - faster-whisper (voice recognition)
# - llama-cpp-python (LLM)
# - selenium (automation)
# - and 15+ other packages
```

### Step 4: Download AI Models

```bash
# Create models directory
mkdir models

# Download Llama-3.2-1B model (770MB)
# Manual download required from:
# https://huggingface.co/TheBloke/Llama-3.2-1B-Instruct-GGUF/blob/main/Llama-3.2-1B-Instruct-Q4_K_M.gguf

# Or use wget (if installed):
wget -P models/ https://huggingface.co/TheBloke/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_K_M.gguf
```

**Model Storage:**
```
VISION_AI/
â””â”€â”€ models/
    â””â”€â”€ Llama-3.2-1B-Instruct-Q4_K_M.gguf  (770MB)
```

**Note:** Whisper model auto-downloads on first run (~1GB)

### Step 5: Run Application

```bash
# Make sure virtual environment is activated
python vision_ai.py
```

---

## ğŸ¯ Usage

### Voice Commands
1. Hold `Ctrl+Windows` key
2. Speak your command
3. Release key when done

### Text Commands
- Type in the input box
- Press Enter or click Send
- Use â†‘â†“ for command history

### Example Commands

**File Operations:**
```
list downloads
open C:\Users\YourName\Documents\file.txt
open %USERPROFILE%\Desktop
find *.pdf
```

**Apps & Web:**
```
open chrome
open notepad
search python tutorial
youtube physics lectures
```

**YouTube (Context-Aware):**
```
browse quantum physics youtube
play first video
next video
pause
```

**Multi-Step (LLM Planning):**
```
list downloads to google keep
open notepad and write hello world
```

**System:**
```
cpu
ram
screenshot
volume 50
time
```

---

## ğŸ—ï¸ Building Executable

Want to create your own `.exe` file?

```bash
# Install PyInstaller
pip install pyinstaller

# Build executable (this takes ~5 minutes)
pyinstaller --onedir --noconsole --name="VISION_AI" --clean vision_ai.py

# Output will be in:
# dist/VISION_AI/
# â”œâ”€â”€ VISION_AI.exe (8.92 MB)
# â””â”€â”€ _internal/ (279 MB)
```

**To distribute:**
1. Zip the entire `dist/VISION_AI/` folder
2. Share the ZIP file (~288 MB compressed)
3. Users extract and run `VISION_AI.exe`
4. Users need to download models separately

---

## ğŸ“¦ Distribution Guide

### For Friends/Users:

**Package Contents:**
```
VISION_AI/
â”œâ”€â”€ VISION_AI.exe          (Main app - 8.92 MB)
â”œâ”€â”€ _internal/             (Dependencies - 279 MB)
â”‚   â”œâ”€â”€ Python runtime
â”‚   â”œâ”€â”€ AI libraries
â”‚   â””â”€â”€ System files
â””â”€â”€ models/                (User must download)
    â””â”€â”€ Llama-3.2-1B-Instruct-Q4_K_M.gguf
```

**Share:**
- âœ… `VISION_AI.exe` 
- âœ… `_internal/` folder
- ğŸ“ Link to model download

**Users need BOTH .exe and _internal folder!**

---

## ğŸ§ª Testing

Run automated test suite:

```bash
# Create test file (if not exists)
# test_suite.py should be in repo

# Run tests
python test_suite.py

# Expected output:
# Total Tests: 33
# Passed: 33
# Success Rate: 100%
```

---

## ğŸ”§ Troubleshooting

### "DLL load failed" or "Module not found"
```bash
# Reinstall dependencies
pip install --force-reinstall -r requirements.txt
```

### Whisper Model Download Fails
```bash
# Manual download alternative
# Visit: https://huggingface.co/guillaumekln/faster-whisper-medium
# Or use different size: base, small, medium, large-v3
```

### LLM Model Not Found
```bash
# Check models folder exists
ls models/

# Verify filename exactly matches:
# Llama-3.2-1B-Instruct-Q4_K_M.gguf
```

### Microphone Not Working
- Check Windows microphone permissions
- Try different hotkey in Settings
- Ensure no other app is using microphone

### High RAM Usage
- Normal for LOW-END: 400-600 MB
- Normal for HIGH-END: 800-1200 MB
- App auto-adapts to your hardware

---

## ğŸ“š Project Structure

```
VISION_AI/
â”œâ”€â”€ vision_ai.py                 # Main application
â”œâ”€â”€ context_manager.py           # Context awareness
â”œâ”€â”€ safety_guard.py              # Command validation
â”œâ”€â”€ file_manager.py              # File operations
â”œâ”€â”€ window_manager.py            # Window controls
â”œâ”€â”€ llm_controller.py            # LLM integration
â”œâ”€â”€ action_executor.py           # Action execution
â”œâ”€â”€ smart_template_matcher.py   # Pattern matching
â”œâ”€â”€ fast_complex_handler.py     # Complex commands
â”œâ”€â”€ agent_memory.py             # Command learning
â”œâ”€â”€ web_search.py               # Web integration
â”œâ”€â”€ device_profiler.py          # Hardware detection
â”œâ”€â”€ requirements.txt            # Dependencies
â”œâ”€â”€ README.md                   # This file
â””â”€â”€ models/                     # AI models
    â””â”€â”€ Llama-3.2-1B-Instruct-Q4_K_M.gguf
```

---

## ğŸ” Privacy

**100% Local AI Processing:**
- âœ… Voice recognition (Whisper) - runs locally
- âœ… LLM planning (Llama) - runs locally
- âœ… Command history - stored locally only
- âœ… No telemetry or data collection

**Optional Internet Usage:**
- ğŸŒ Web search (DuckDuckGo) - only when you use search commands
- ğŸŒ YouTube automation - only when you browse YouTube
- ğŸŒ Model downloads - one-time initial setup

See [PRIVACY.md](PRIVACY.md) for full details.

---

## ğŸ¤ Contributing

Contributions are welcome! 

1. Fork the repository
2. Create feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **Whisper** - OpenAI (via faster-whisper)
- **Llama** - Meta AI (via llama-cpp-python)
- **CustomTkinter** - TomSchimansky
- **Selenium** - SeleniumHQ
- All other open-source contributors

---

## ğŸ“ Support

- **Issues:** [GitHub Issues](https://github.com/YOUR_USERNAME/VISION_AI/issues)
- **Discussions:** [GitHub Discussions](https://github.com/YOUR_USERNAME/VISION_AI/discussions)

---

## ğŸš€ Roadmap

- [x] Voice recognition (Whisper)
- [x] LLM planning (Llama)
- [x] Adaptive performance
- [x] Memory system
- [x] Web search
- [ ] Computer vision (PaddleOCR)
- [ ] Multi-monitor support
- [ ] Plugin system
- [ ] Cloud sync (optional)

---

**Made with â¤ï¸ for productivity and privacy**

*VISION AI - See what's possible with local AI*